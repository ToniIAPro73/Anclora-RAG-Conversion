# Anclora AI RAG v1.0

Este repositorio contiene todo lo necesario para poder crear tu propia secretaria hecha con Inteligencia Artificial, todo gracias al RAG de Basdonax AI, que utiliza los modelos open source de Meta y de Microsoft: `Llama3-7b` y `Phi3-4b` para de esta forma darte la posibilidad de subir tus documentos y hacer consultas a los mismos. Esto fue creado para poder facilitarle la vida a las personas con la IA.

## Informaci√≥n del Proyecto

### Resumen

Anclora AI RAG es un sistema de Generaci√≥n Aumentada por Recuperaci√≥n (RAG) que permite a los usuarios subir documentos y realizar consultas sobre ellos utilizando modelos de lenguaje de c√≥digo abierto como Llama3-7b y Phi3-4b. Actualmente la experiencia est√° optimizada para trabajar en espa√±ol e ingl√©s, y se ha iniciado un plan de expansi√≥n para incorporar m√°s idiomas en pr√≥ximas versiones. La soluci√≥n utiliza Docker para la contenerizaci√≥n.

### Estructura

- **app/**: C√≥digo principal de la aplicaci√≥n incluyendo la interfaz de Streamlit, procesamiento de documentos e implementaci√≥n RAG
- **docs/**: Archivos de documentaci√≥n del proyecto
- **.vscode/**: Configuraci√≥n de VS Code
- **docker-compose.yml**: Stack base (CPU) con Ollama, Chroma, UI/API y observabilidad.
- **docker-compose.gpu.yml**: Overlay opcional para habilitar GPU (reservas NVIDIA y diagn√≥stico con `nvidia-smi`).
- **docker-compose_sin_gpu.yml**: Variante alternativa hist√≥rica sin GPU (modelo Phi3).

### Componentes principales del repositorio

- **Interfaz Streamlit (`app/Inicio.py` y `app/pages/`)**: Proporciona el chat principal y la gesti√≥n de archivos para la base de conocimiento a trav√©s del puerto `8080`. Aqu√≠ se orquestan las llamadas al m√≥dulo RAG, se gestionan los estados de sesi√≥n y se aplican los estilos personalizados.
- **API FastAPI (`app/api_endpoints.py`)**: Expone endpoints REST para integraciones externas en el puerto `8081`, reutilizando la misma l√≥gica de recuperaci√≥n y generaci√≥n que la interfaz. Incluye operaciones de consulta, ingesta y administraci√≥n de documentos.
- **Scripts de arranque (`open_rag.sh` y `open_rag.bat`)**: Automatizan el levantamiento del stack Docker desde distintas plataformas. Solo requieren actualizar la ruta del proyecto antes de ejecutar `docker compose up -d` (aceptan par√°metros adicionales como `--profile` o `-f`).

### Lenguaje y Entorno

**Lenguaje**: Python 3.11  
**Framework**: Streamlit  
**Sistema de Construcci√≥n**: Docker  
**Gestor de Paquetes**: pip  

### Dependencias

**Dependencias Principales** (Actualizadas a Pydantic v2):

- langchain>=0.2.0
- langchain-community>=0.2.0
- chromadb==0.5.15
- streamlit>=1.28.0
- sentence_transformers
- PyMuPDF==1.23.5
- fastapi>=0.111.0
- uvicorn[standard]
- python-multipart
- pydantic>=2.8.0
- llama-parse>=0.4.0
- ollama (v√≠a Docker)

**Servicios Externos**:

- ChromaDB (base de datos vectorial)
- Ollama (servicio LLM)

#### Modelos de embeddings

El gestor de embeddings permite asignar un modelo diferente por dominio (`documents`, `code`, `multimedia`). Por defecto se
usa `all-MiniLM-L6-v2`, pero puedes personalizarlo mediante variables de entorno o un archivo YAML:

- `EMBEDDINGS_MODEL_NAME`: modelo por defecto aplicado a todos los dominios si no hay overrides.
- `EMBEDDINGS_MODEL_<DOMINIO>`: valor espec√≠fico por dominio (`EMBEDDINGS_MODEL_CODE`, `EMBEDDINGS_MODEL_MULTIMEDIA`, etc.).
- `EMBEDDINGS_CONFIG_FILE`: ruta a un YAML con la estructura:

```yaml
default_model: sentence-transformers/all-MiniLM-L6-v2
domains:
  documents: sentence-transformers/all-mpnet-base-v2
  code: sentence-transformers/all-mpnet-base-v2
  multimedia: intfloat/multilingual-e5-large
```

Ejemplo de configuraci√≥n en `docker-compose.yml` asignando modelos distintos:

```yaml
environment:
  - MODEL=llama3
  - EMBEDDINGS_MODEL_NAME=sentence-transformers/all-MiniLM-L6-v2
  - EMBEDDINGS_MODEL_CODE=sentence-transformers/all-mpnet-base-v2
  - EMBEDDINGS_MODEL_MULTIMEDIA=intfloat/multilingual-e5-large
```

Para comparar r√°pidamente el rendimiento de distintos modelos se incluye el script `scripts/eval_embeddings.py`:

```bash
# Usa los valores definidos en la configuraci√≥n del gestor
python scripts/eval_embeddings.py

# Compara m√∫ltiples modelos en una sola corrida
python scripts/eval_embeddings.py --models sentence-transformers/all-mpnet-base-v2 intfloat/multilingual-e5-large
```

Si cambias el modelo de embeddings o las dependencias relacionadas, recuerda regenerar las im√°genes ejecutando `docker compose
build --no-cache` para los servicios `ui` y `api`.

### Docker

**Dockerfile**: app/Dockerfile  
**Im√°genes**:

- ollama/ollama:latest (servicio LLM)
- chromadb/chroma:0.5.1.dev111 (Base de datos vectorial)
- nvidia/cuda:12.3.1-base-ubuntu20.04 (solo al combinar con `docker-compose.gpu.yml`)
- Imagen UI/API personalizada construida desde `./app`

**Configuraci√≥n**:

- Overlay opcional para habilitar GPU cuando se combine `docker-compose.yml` con `docker-compose.gpu.yml`
- Configuraci√≥n solo CPU disponible para el modelo Phi3
- Volumen persistente para ChromaDB
- Variables de entorno para selecci√≥n de modelo y configuraci√≥n de embeddings

#### Variables de entorno para ChromaDB

Los servicios `ui` y `api` consumen ChromaDB mediante las variables de entorno `CHROMA_HOST` y `CHROMA_PORT`, definidas en los archivos `docker-compose*.yml` con los valores por defecto `chroma` y `8000`. Estos valores permiten que ambos servicios descubran autom√°ticamente al contenedor `chroma` cuando el stack se ejecuta con Docker Compose.

Si ejecutas la aplicaci√≥n fuera de Docker (por ejemplo, para desarrollo local), establece las variables antes de iniciar Streamlit o la API para apuntar al host correspondiente:

```bash
export CHROMA_HOST=localhost
export CHROMA_PORT=8000
streamlit run app/Inicio.py
# o
uvicorn app.api_endpoints:app --reload --port 8081
```

Tambi√©n puedes ajustar `CHROMA_HOST` y `CHROMA_PORT` a los valores de cualquier instancia remota de ChromaDB que quieras reutilizar.

## üöÄ Instalaci√≥n y Configuraci√≥n

### Prerrequisitos

- Python 3.11.8 (recomendado)
- Docker y Docker Compose
- Git

### Configuraci√≥n del Entorno

1. **Clonar el repositorio:**

   ```bash
   git clone https://github.com/ToniIAPro73/basdonax-ai-rag.git
   cd basdonax-ai-rag
   ```

2. **Configurar variables de entorno:**

   ```bash
   cp .env.example .env
   # Editar .env con tus claves API
   ```

3. **Opci√≥n A: Usar entorno virtual (Recomendado para desarrollo):**

   ```bash
   # Crear entorno virtual con Python 3.11
   python -m venv venv_rag --python=python3.11

   # Activar entorno virtual (Windows)
   activate_venv.bat

   # Activar entorno virtual (Linux/Mac)
   source venv_rag/bin/activate

   # Instalar dependencias
   pip install -r app/requirements.txt

   # Verificar instalaci√≥n
   python test_environment.py
   ```

4. **Opci√≥n B: Ejecutar con Docker:**

   ```bash
   docker-compose up -d
   ```

### Dependencias Actualizadas

El proyecto ahora utiliza **Pydantic v2** y versiones actualizadas de las dependencias principales:

- **langchain>=0.2.0** (Framework LLM)
- **langchain-community>=0.2.0** (Extensiones de LangChain)
- **pydantic>=2.8.0** (Validaci√≥n de datos)
- **chromadb==0.5.15** (Base de datos vectorial)
- **llama-parse>=0.4.0** (Procesamiento de documentos complejos en ZIP)
- **fastapi>=0.111.0** (API REST)
- **streamlit>=1.28.0** (Interfaz web)

### Archivos Principales

**Punto de Entrada**: app/Inicio.py
**M√≥dulos Clave**:

- app/common/langchain_module.py: Implementaci√≥n RAG
- app/common/ingest_file.py: Procesamiento de documentos
- app/common/assistant_prompt.py: Plantilla de prompt para LLM
- app/pages/Archivos.py: Interfaz de gesti√≥n de archivos
- app/api_endpoints.py: API REST para agentes y automatizaciones

## Roadmap y contribuci√≥n

El detalle de fases, √©picas y tareas priorizadas se encuentra en el [backlog del roadmap](docs/backlog.md). All√≠ se listan los m√≥dulos involucrados (`app/common/*`, `app/pages/*`, `docker-compose*.yml`, etc.), los criterios de aceptaci√≥n y las dependencias principales para cada bloque de trabajo. Rev√≠salo antes de abrir issues o Pull Requests para mantener el alineamiento con la hoja de ruta.

### Documentaci√≥n adicional

- [Gu√≠a de Integraci√≥n / Integration Guide](docs/integration-guide.md): Pasos para consumir la API, usar el cliente Python oficial y conectar agentes (LangChain, AutoGen) con recomendaciones de encoding y manejo de Unicode.

### Configuraci√≥n de credenciales

Los scripts `open_rag.sh` (Linux/macOS) y `open_rag.bat` (Windows) cargan autom√°ticamente el archivo `.env` si existe. Si no defines `ANCLORA_API_TOKENS` ni `ANCLORA_JWT_SECRET`, se reutilizar√° el valor de `ANCLORA_DEFAULT_API_TOKEN` y se mostrar√° una advertencia indicando que se usar√° el token por defecto. Esto permite levantar la pila r√°pidamente en entornos locales manteniendo la opci√≥n de reforzar la seguridad en producci√≥n.

Para inyectar credenciales reales, copia el archivo `.env.example` a `.env` y reemplaza los valores por los de tu entorno:

```bash
cp .env.example .env
# Edita .env y define ANCLORA_API_TOKENS o ANCLORA_JWT_SECRET con tus credenciales.
```

En producci√≥n se recomienda definir tus propios tokens o secretos JWT para deshabilitar el token de ejemplo.

### Informaci√≥n legal y cumplimiento

- [T√©rminos y Condiciones de Uso](docs/legal/terms.md): reglas de uso de la plataforma, pol√≠tica de consentimiento y proceso de verificaci√≥n de derechos antes de cada conversi√≥n.
- [Pol√≠tica de Privacidad](docs/legal/privacy.md): tratamiento de datos personales, conservaci√≥n de evidencias de consentimiento y medidas de seguridad recomendadas.

### Uso

La aplicaci√≥n se ejecuta en <http://localhost:8080> y la API REST est√° disponible en <http://localhost:8081>. Desde la UI y la API se ofrece:

- Interfaz de chat para consultar documentos
- Interfaz de carga de archivos para a√±adir documentos a la base de conocimiento
- Selecci√≥n de idioma para la interfaz (ver nota m√°s abajo)
- Gesti√≥n de documentos (ver y eliminar)
- Endpoints para integraciones externas (consultas, ingesti√≥n y listado de archivos)

### Descripci√≥n textual de la interfaz principal

![Diagrama textual de la vista de conversaci√≥n](docs/images/conversor_page.svg)

- **Barra lateral**: muestra el t√≠tulo de la aplicaci√≥n y el selector de idioma. Al cambiar la opci√≥n se actualizan los textos y prompts en tiempo real.
- **Panel de conversaci√≥n**: conserva el historial de mensajes del usuario y del asistente, con indicadores de carga mientras se generan las respuestas.
- **Entrada del chat**: campo fijado en la parte inferior que valida mensajes vac√≠os o con m√°s de 1000 caracteres antes de enviarlos.

### Ejemplos de uso de la API REST

Consulta en espa√±ol preservando acentos:

```bash
curl -X POST "http://localhost:8081/chat" \
  -H "Authorization: Bearer your-api-key-here" \
  -H "Content-Type: application/json" \
  -d '{
        "message": "¬øCu√°l es el estado del informe trimestral?",
        "language": "es",
        "max_length": 600
      }'
```

Consulta en ingl√©s con caracteres √±/√°:

```bash
curl -X POST "http://localhost:8081/chat" \
  -H "Authorization: Bearer your-api-key-here" \
  -H "Content-Type: application/json" \
  -d '{
        "message": "Please summarize the jalape√±o market update on D√≠a 1",
        "language": "en",
        "max_length": 600
      }'
```

Carga de archivos desde la terminal:

```bash
curl -X POST "http://localhost:8081/upload" \
  -H "Authorization: Bearer your-api-key-here" \
  -F "file=@revisi√≥n_t√©cnica.pdf"
```

## Requisitos previos

- Docker o Docker desktop: <https://www.docker.com/products/docker-desktop/>
- **(opcional)** Tarjeta gr√°fica RTX

## Instalaci√≥n

### Elecci√≥n del modelo de datos (LLM)

Antes de comenzar con la instalaci√≥n, define si ejecutar√°s el stack solo con CPU o si habilitar√°s GPU. El archivo base `docker-compose.yml` funciona en ambos escenarios (por defecto el servicio `ollama` expone el modelo `llama3`, que tambi√©n puede ejecutarse en CPU). Si prefieres un modelo m√°s liviano puedes reutilizar la variante `docker-compose_sin_gpu.yml` (modelo `phi3`).

- **Solo CPU:** `docker compose up -d` levanta `ollama`, `chroma`, `ui`, `api`, Prometheus y Grafana.
- **Con GPU NVIDIA:** combina los archivos ejecutando `docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d`. El segundo archivo agrega las reservas de GPU y un contenedor auxiliar (`gpu-diagnostics`) que ejecuta `nvidia-smi` para validar el acceso a la tarjeta.

#### Docker Installation

Tenemos que tener Docker o Docker Desktop instalado, te recomiendo ver este video para instalar todo: <https://www.youtube.com/watch?v=ZyBBv1JmnWQ>

Una vez instalado y prendido el Docker Desktop si lo estamos utilizando, vamos a ejecutar en esta misma carpeta:

```bash
docker compose up
```

La primera vez vamos a tener que esperar a que todo se instale correctamente, va a tardar unos cuantos minutos en ese paso.

Ahora tenemos que instalarnos nuestro modelo LLM, si tenemos una GPU que pueda soportar vamos a ejecutar el comando para traernos Llama3, sino va a ser Phi3 (si queremos utilizar otro modelo, en esta pagina: <https://ollama.com/library> tenes la lista de todos los modelos open source posibles en esta p√°gina, recorda que seguramente vayas a tener que hacer cambios en la prompt si cambias el modelo), ejecutamos:

```bash
docker ps
```

Te va a aparecer algo como esto:

```text
CONTAINER ID   IMAGE                    COMMAND                  CREATED              STATUS              PORTS                    NAMES
696d2e45ce7c   ui                       "/bin/sh -c 'streaml‚Ä¶"   About a minute ago   Up About a minute   0.0.0.0:8080->8080/tcp   ui-1
28cf32abee50   ollama/ollama:latest     "/bin/ollama serve"      About a minute ago   Up About a minute   11434/tcp                ollama-1
ec09714c3c86   chromadb/chroma:latest   "/docker_entrypoint.‚Ä¶"   About a minute ago   Up About a minute   0.0.0.0:8000->8000/tcp   chroma-1
```

En esta parte ten√©s que copiar el `CONTAINER ID` de la imagen llamada `ollama/ollama:latest` y utilizarla para este comando:

```bash
docker exec [CONTAINER ID] ollama pull [nombredelmodelo]
```

Un ejemplo con `Llama3-7b` y mi `CONTAINER ID`

```bash
docker exec 28cf32abee50 ollama pull llama3
```

Un ejemplo con `Phi3-4b` y mi `CONTAINER ID`

```bash
docker exec 28cf32abee50 ollama pull phi3
```

Ahora vamos a tener que esperar a que se descargue el modelo, una vez hecho esto solo nos queda modificar la prompt:

Esto se va a hacer a nuestro gusto en el archivo `./app/common/assistant_prompt.py`.

Una vez hecho todo lo anterior solo queda un paso: que entremos al siguiente link: <http://localhost:8080> para poder utilizar el RAG.

## Pruebas automatizadas

Ejecuta la bater√≠a de pruebas localmente con:

```bash
pytest
```

Para correr √∫nicamente la nueva suite de regresi√≥n que valida los flujos del asistente utiliza:

```bash
pytest tests/regression
```

## Selecci√≥n de idioma

Actualmente la interfaz y las respuestas del asistente est√°n validadas para espa√±ol (por defecto) e ingl√©s. Puedes alternar entre ambos idiomas desde el selector de la barra lateral, y la preferencia se mantendr√° durante toda la sesi√≥n.

### Roadmap de soporte multiling√ºe

El equipo est√° trabajando para ampliar progresivamente la cobertura ling√º√≠stica:

1. **Portugu√©s**: previsto para la siguiente iteraci√≥n, incluyendo traducciones de UI y prompts especializados.
2. **Franc√©s y Alem√°n**: planeados tras estabilizar portugu√©s, con validaci√≥n de m√©tricas de calidad antes de lanzamiento.
3. **Otros idiomas**: se evaluar√°n seg√∫n demanda, priorizando documentaci√≥n y prompts espec√≠ficos para cada caso.

## ¬øComo ejecutarlo posteriormente instalado y una vez lo cerremos?

Tenemos que dejarnos en el escritorio el archivo de `open_rag.bat` si estamos en Windows y si estamos en Mac/Linux el `open_rag.sh`

Ahora tenemos que abrirlo y modificarlo, tenemos que agregar la ruta donde hicimos/tenemos el `docker-compose.yml`, por ejemplo mi ruta es:

```text
C:\Users\fcore\OneDrive\Desktop\Basdonax\basdonax-rag>
```

> üí° Consulta la [gu√≠a detallada en `docs/guia_open_rag.md`](docs/guia_open_rag.md) para revisar prerrequisitos, pasos de ejecuci√≥n y soluciones a errores comunes al usar `open_rag.sh` y `open_rag.bat`.

Entonces en mi caso va a ser as√≠ el `open_rag.bat` (el .sh es lo mismo):

```batch
cd C:\Users\fcore\OneDrive\Desktop\Basdonax\basdonax-rag
docker compose up -d
```

Si necesitas habilitar GPU desde estos scripts, agrega los archivos adicionales antes del comando `up`, por ejemplo:

```batch
docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d
```

Ahora mientras que tengamos el Docker/Docker Desktop prendido y mientras que ejecutemos este archivo vamos a poder acceder al RAG en este link: <http://localhost:8501>

Pr√≥ximo paso: disfrutar
